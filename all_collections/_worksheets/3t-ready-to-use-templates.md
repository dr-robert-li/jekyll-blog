# Ready-to-Use Templates for 3T Team Implementation

**Templates transform abstract frameworks into concrete action.** These templates work across industries and use cases—customize the specific questions and metrics, but keep the structure.

## AI Maturity Self-Assessment Template (MIT CISR Framework)

**Example AI Maturity Self-Assessment Template (MIT CISR Framework)** covering six dimensions with 30 yes/no questions. Here are some example questions - they can be modified to suit your organization and offerings:

+--------------------------------+---------------------------------------------------------------------------------+
| Dimension                      | Assessment Questions                                                            |
+================================+=================================================================================+
| **Strategy and Vision**        | 1. Is there a written AI vision or strategy aligned with overall business      |
|                                | objectives?                                                                     |
|                                |                                                                                 |
|                                | 2. Have high-value AI use cases supporting key goals been identified?          |
|                                |                                                                                 |
|                                | 3. Does executive leadership actively support and sponsor AI initiatives?      |
|                                |                                                                                 |
|                                | 4. Are there clear, quantified success metrics for AI projects?                |
|                                |                                                                                 |
|                                | 5. Is there a realistic, phased roadmap for AI adoption and scaling?           |
+--------------------------------+---------------------------------------------------------------------------------+
| **Data Infrastructure**        | 6. Is quality, fit-for-purpose data readily available for critical AI use      |
|                                | cases?                                                                          |
|                                |                                                                                 |
|                                | 7. Are core data sources consolidated rather than siloed across teams?         |
|                                |                                                                                 |
|                                | 8. Does the organization follow formal data governance and stewardship         |
|                                | policies?                                                                       |
|                                |                                                                                 |
|                                | 9. Are data privacy and security controls consistently enforced?               |
|                                |                                                                                 |
|                                | 10. Is there a standardized process for keeping data clean, well-structured,   |
|                                | and up to date?                                                                 |
+--------------------------------+---------------------------------------------------------------------------------+
| **Technology and               | 11. Is current IT infrastructure (on-prem/cloud) technically equipped to run   |
| Infrastructure**               | AI/ML workloads?                                                                |
|                                |                                                                                 |
|                                | 12. Does the organization leverage cloud or hybrid platforms for scalable AI   |
|                                | resource needs?                                                                 |
|                                |                                                                                 |
|                                | 13. Can AI tools integrate smoothly with existing core business systems?       |
|                                |                                                                                 |
|                                | 14. Are there sufficient computing resources and storage for current and       |
|                                | planned AI projects?                                                            |
|                                |                                                                                 |
|                                | 15. Is the security architecture robust and regularly reviewed as AI scales?   |
+--------------------------------+---------------------------------------------------------------------------------+
| **Talent and Skills**          | 16. Do most employees have a basic understanding of AI concepts and             |
|                                | opportunities?                                                                  |
|                                |                                                                                 |
|                                | 17. Is there sufficient in-house expertise in data science, ML, or AI          |
|                                | engineering?                                                                    |
|                                |                                                                                 |
|                                | 18. Are employees encouraged and willing to acquire AI-related skills?         |
|                                |                                                                                 |
|                                | 19. Does the organization have or plan formal AI upskilling/training programs? |
|                                |                                                                                 |
|                                | 20. Has a hiring or external consulting strategy addressed core AI skill gaps? |
+--------------------------------+---------------------------------------------------------------------------------+
| **Governance and Ethics**      | 21. Are responsibilities and frameworks for AI governance established and       |
|                                | documented?                                                                     |
|                                |                                                                                 |
|                                | 22. Does the organization have published AI ethical guidelines or principles?  |
|                                |                                                                                 |
|                                | 23. Are risk management and regulatory requirements assessed for each AI       |
|                                | initiative?                                                                     |
|                                |                                                                                 |
|                                | 24. Are there checks in place for bias, fairness, and transparency in AI       |
|                                | outputs?                                                                        |
|                                |                                                                                 |
|                                | 25. Can AI-driven decisions be clearly explained to non-technical stakeholders?|
+--------------------------------+---------------------------------------------------------------------------------+
| **Organizational Culture**     | 26. Does leadership promote innovation and calculated risk-taking with AI?     |
|                                |                                                                                 |
|                                | 27. Are teams open and responsive to change driven by new technology?          |
|                                |                                                                                 |
|                                | 28. Is there an organizational willingness to learn from failed or suboptimal  |
|                                | AI projects?                                                                    |
|                                |                                                                                 |
|                                | 29. Do different departments actively collaborate on AI initiatives?           |
|                                |                                                                                 |
|                                | 30. Do employees tend to view AI as an opportunity, not a threat, for the      |
|                                | business?                                                                       |
+--------------------------------+---------------------------------------------------------------------------------+

**Scoring Guide:**
- 0–8: Foundational Stage
- 9–16: Developing Stage
- 17–24: Mature Stage
- 25–30: Leading (AI Future-Ready) Stage

## Pilot Program Charter Template

| Section               | Fields/Details |
|-----------------------|---------------|
| **Project Overview**  | Project Name, Executive Sponsor, Project Manager, Start Date, End Date, Total Budget |
| **Business Case**     | Problem Statement (describe current challenge), Strategic Alignment (link to company goals), Expected Benefits (quantified), Success Criteria and Metrics |
| **Scope**             | In-Scope Items (specifically included), Out-of-Scope Items (explicitly excluded), Assumptions, Constraints |
| **Stakeholders**      | Name, Role, Interest Level, Engagement Strategy for each |
| **Key Deliverables**  | Deliverable, Due Date, Owner, Success Criteria |
| **Success Metrics/KPIs** | Metric, Baseline, Target, Measurement Frequency |
| **Team and Resources**| Name, Role, Time Commitment (per team member), Technology Requirements, Budget Allocation |
| **Risk Register**     | Risk Description, Likelihood, Impact, Mitigation Strategy, Owner |
| **Communication Plan**| Audience, Message, Frequency, Channel, Owner |
| **Approval Signatures** | Executive Sponsor, Project Manager, IT Lead, Finance Lead, Date |

## Quarterly Red-Teaming Checklist

| Month / Activity              | Checklist Items |
|-------------------------------|----------------|
| **Month 1: Assessment and Planning** | Review prior quarter's incidents; Update risk assessment (reflect new AI deployments); Schedule red-teaming exercises; Audit marketing data privacy practices; Review consent management systems; Test privacy request fulfillment procedures |
| **Month 2: Active Testing**   | Red-teaming exercises (prompt injection, data poisoning, adversarial examples); Test model performance and drift detection; Evaluate bias/fairness across demographics; Information integrity assessment (Harari, 2024); Test incident response via tabletop exercise; Review third-party/vendor compliance |
| **Month 3: Review and Remediation** | Analyze results for patterns; Document findings/remediation steps with owners/due dates; Update protocols based on learnings; Refine containment strategies (Suleyman, 2023); Prepare quarterly report (metrics/trends) for leadership; Plan next quarter's testing focus areas |
| **Continuous**                | Monitor AI system performance metrics daily; Log incidents in tracking system immediately; Weekly privacy rights request review; Monthly employee training on AI safety; Update documentation as systems change |

## Victory Journal Daily Template

| Field              | Prompts/Structure |
|--------------------|------------------|
| **Date**           |                  |
| **Morning (3 min)**| - Today's top 3 priorities; - How today advances our mission; - What obstacles might arise? |
| **Evening (4 min)**| - Victories (big and small); - What worked well?; - Tomorrow's focus; - Key learning from today |

## Weekly Victory Sharing Circle Template

| Segment               | Duration | Activities |
|-----------------------|----------|------------|
| **Opening**           | 2 min    | Set purpose and context |
| **Victory Sharing**   | 20 min   | Each person shares 1–2 victories, brief context |
| **Team Celebration**  | 5 min    | Recognize patterns and collective wins |
| **Photo for Victory Wall** | 3 min | Take group photo to capture the moment |

## Sprint Demo Agenda Template

| Segment                 | Duration    | Activities |
|-------------------------|-------------|------------|
| **Context**             | 5 min       | Sprint goal reminder, key metrics from last sprint |
| **Demonstrations**      | 25 min      | 5 stories × 5 min: Acceptance criteria review, live user value demo, Q&A for each story |
| **Q&A and Feedback**    | 10 min      | Gather overall impressions and concerns |
| **Next Sprint Preview** | 5 min       | Preview upcoming priorities |

## Human-in-the-Loop Review Form (Google Forms)

| Field                | Type/Options |
|----------------------|--------------|
| Reviewer Name        | Dropdown     |
| Item ID              | Short answer |
| AI Output Summary    | Paragraph    |
| AI Confidence Score  | Linear scale (0–100) |
| Approval Decision    | Multiple choice (Approve / Reject / Request Changes) |
| Issues Found         | Checkboxes (Accuracy, Completeness, Compliance, Bias, Other) |
| Specific Feedback    | Paragraph    |
| Recommended Action   | Paragraph    |
| Review Time Spent    | Short answer (minutes) |
| Escalation Needed    | Yes/No (if yes, explanation required) |

## 30-60-90 Day Success Metrics Dashboard

| Metric Group      | Metrics/Fields |
|-------------------|----------------|
| **Pilot Program Health** | Days elapsed, Current phase, On track / At risk / Off track, Blockers count, Risk count |
| **Adoption Metrics**     | Pilot users trained, Daily active users, Adoption rate (%), Feature utilization rate |
| **Performance Metrics**  | AI accuracy/confidence, System uptime (%), Avg. response time, Errors logged count |
| **Business Impact**      | Time saved/user weekly, Cost savings, User satisfaction score, Productivity improvement (%) |
| **Milestones**           | Milestone, Target date, Actual date, Status, Notes |

*Review this dashboard weekly with the pilot team, bi-weekly with stakeholders, and monthly with leadership.*

## Quarterly OKR Template

| Field                   | Structure/Prompts |
|-------------------------|-------------------|
| Objective               | Clear, qualitative statement of achievement |
| Key Results             | 3–5 quantitative measures of success |
| Owner                   | Person accountable |
| Quarter                 | Q1/Q2/Q3/Q4 Year |
| Current Status          | On track / At risk / Off track |
| Overall Score           | 0.0–1.0 |
| Individual KR Details   | Description, Baseline, Target, Actual, Score (0.0–1.0), Status |
| Progress Updates        | Date, Update, Challenges, Next steps |
| Retrospective           | What worked well, What to improve, Learnings for next quarter |

**Score 0.7–0.8 is target range; consistently scoring 1.0 means OKRs aren't ambitious enough.**

# Critical Warning Signs and Course Corrections for AI Initiatives

**Most failed AI initiatives show warning signs months before collapse.** Organizations that recognize and respond to these signals early can course-correct; those that ignore them waste significant resources. Monitor these indicators monthly and take immediate action when you see patterns.

## Warning Sign 1: Low Adoption After 4 Weeks

**Indicator:** If pilot users aren't using the AI tool at least 3 times per week after 4 weeks, the implementation is failing.

**Root Causes:**
- Tool doesn't solve real problem (users work around it)
- Tool too difficult to use (friction exceeds value)
- Inadequate training or support (users don't understand how to get value)
- Integration gaps (tool doesn't fit workflow)
- Performance issues (tool too slow, inaccurate, or unreliable)

**Course Correction:**
- Conduct in-place interviews with non-users to understand barriers
- Simplify the use case by reducing scope or improve UX
- Add hands-on training sessions
- Fix integration issues
- If fundamental mismatch, pivot to different use case, replace, or eliminate entirely

## Warning Sign 2: Declining Use After Initial Spike

**Indicator:** If users try the tool then abandon it, you have a value realization problem and the pilot to established stack process was rushed or improperly conducted (there may have been bias).

**Root Causes:**
- Initial novelty wears off without sustained value
- Outputs require too much correction (human oversight burden too high)
- Users don't trust AI outputs (accuracy issues)
- Manual process still required (automation incomplete)
- No reinforcement or reminder to use tool

**Course Correction:**
- Identify power users and understand what makes them stick and whether this is scalable i.e. others can use it in the same way
- Improve output quality through retraining or tuning
- Implement periodic "check-ins" or reminders
- Add features that increase stickiness
- Celebrate wins publicly to drive adoption

## Warning Sign 3: High Error Rates (Over 15% Requiring Correction)

**Indicator:** If more than 15% of AI outputs require significant human correction, the system isn't ready for scaling.

**Root Causes:**
- Training data doesn't match use case well
- Model not suited for task complexity
- Edge cases not handled properly
- Insufficient testing before pilot launch
- Confidence thresholds set incorrectly

**Course Correction:**
- Expand training data with real-world examples
- Adjust confidence thresholds (lower threshold increases human review, improves accuracy)
- Implement better error handling for edge cases
- Consider different model or approach
- Narrow the use case to areas where accuracy is higher

## Warning Sign 4: Organizational Resistance or Pushback

**Indicator:** If stakeholders or departments resist the initiative, you have a change management failure.

**Root Causes:**
- Insufficient communication about benefits
- Fear of job displacement
- "Not invented here" syndrome
- Previous failed technology initiatives
- Lack of visible leadership support

**Course Correction:**
- Increase communication frequency and transparency
- Reframe AI as augmentation not replacement
- Involve resistors in design decisions (give them ownership)
- Share early wins and user testimonials
- Ensure executive sponsors are visibly engaged

## Warning Sign 5: Budget Overruns (Over 20% Above Plan)

**Indicator:** If costs exceed projections significantly, you have planning or scope problems.

**Root Causes:**
- Underestimated usage-based pricing
- Excessive AI technology stack
- Scope creep without budget adjustment
- Hidden integration or customization costs
- Unplanned training or support needs
- Vendor pricing changes

**Course Correction:**
- Implement usage monitoring and optimization
- Revisit scope and re-align budget or cut features
- Audit your technology stack
- Renegotiate vendor contracts if possible
- Forecast costs monthly and adjust expectations
- Consider switching to options with more acceptable pricing models

## Warning Sign 6: Technical Debt Accumulating

**Indicator:** If workarounds, patches, and "temporary" solutions are piling up, you're building on unstable foundation.

**Root Causes:**
- Moving too fast without proper architecture
- Insufficient testing before deployment
- Lack of documentation
- Inadequate technical expertise
- No time allocated for refactoring

**Course Correction:**
- Schedule dedicated technical debt sprint (1-2 weeks)
- Document all systems and processes properly
- Bring in technical consultant for architecture review
- Slow down new feature development temporarily
- Invest in training for technical team

## Warning Sign 7: Regulatory or Compliance Issues

**Indicator:** If you discover privacy, security, or compliance gaps, stop immediately.

**Root Causes:**
- Inadequate understanding of regulatory requirements
- Insufficient legal review before deployment
- Data governance policies not enforced
- Third-party vendors not properly vetted
- Lack of audit trails or documentation

**Course Correction:**
- Pause deployment until compliance issues resolved
- Conduct full compliance audit with legal counsel
- Implement proper data governance immediately
- Review all vendor contracts and DPAs
- Document all AI decisions and human oversight
- Establish compliance review checkpoints
