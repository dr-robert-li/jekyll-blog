# Ready-to-Use Templates for 3T Team Implementation

**Templates transform abstract frameworks into concrete action.** These templates work across industries and use cases—customize the specific questions and metrics, but keep the structure.

## AI Maturity Self-Assessment Template (MIT CISR Framework)

**Example AI Maturity Self-Assessment Template (MIT CISR Framework)** covering six dimensions with 30 yes/no questions. Here are some example questions - they can be modified to suit your organization and offerings:

| Dimension                  | Assessment Questions                                                                                                                                           |
|----------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Strategy and Vision**    | 1. Is there a written AI vision or strategy aligned with overall business objectives?<br>2. Have high-value AI use cases supporting key goals been identified?<br>3. Does executive leadership actively support and sponsor AI initiatives?<br>4. Are there clear, quantified success metrics for AI projects?<br>5. Is there a realistic, phased roadmap for AI adoption and scaling? |
| **Data Infrastructure**    | 6. Is quality, fit-for-purpose data readily available for critical AI use cases?<br>7. Are core data sources consolidated rather than siloed across teams?<br>8. Does the organization follow formal data governance and stewardship policies?<br>9. Are data privacy and security controls consistently enforced?<br>10. Is there a standardized process for keeping data clean, well-structured, and up to date? |
| **Technology and Infrastructure** | 11. Is current IT infrastructure (on-prem/cloud) technically equipped to run AI/ML workloads?<br>12. Does the organization leverage cloud or hybrid platforms for scalable AI resource needs?<br>13. Can AI tools integrate smoothly with existing core business systems?<br>14. Are there sufficient computing resources and storage for current and planned AI projects?<br>15. Is the security architecture robust and regularly reviewed as AI scales? |
| **Talent and Skills**      | 16. Do most employees have a basic understanding of AI concepts and opportunities?<br>17. Is there sufficient in-house expertise in data science, ML, or AI engineering?<br>18. Are employees encouraged and willing to acquire AI-related skills?<br>19. Does the organization have or plan formal AI upskilling/training programs?<br>20. Has a hiring or external consulting strategy addressed core AI skill gaps? |
| **Governance and Ethics**  | 21. Are responsibilities and frameworks for AI governance established and documented?<br>22. Does the organization have published AI ethical guidelines or principles?<br>23. Are risk management and regulatory requirements assessed for each AI initiative?<br>24. Are there checks in place for bias, fairness, and transparency in AI outputs?<br>25. Can AI-driven decisions be clearly explained to non-technical stakeholders? |
| **Organizational Culture** | 26. Does leadership promote innovation and calculated risk-taking with AI?<br>27. Are teams open and responsive to change driven by new technology?<br>28. Is there an organizational willingness to learn from failed or suboptimal AI projects?<br>29. Do different departments actively collaborate on AI initiatives?<br>30. Do employees tend to view AI as an opportunity, not a threat, for the business? |

**Scoring Guide:**
- 0–8: Foundational Stage
- 9–16: Developing Stage
- 17–24: Mature Stage
- 25–30: Leading (AI Future-Ready) Stage

## Pilot Program Charter Template

| Section               | Fields/Details |
|-----------------------|---------------|
| **Project Overview**  | Project Name, Executive Sponsor, Project Manager, Start Date, End Date, Total Budget |
| **Business Case**     | Problem Statement (describe current challenge), Strategic Alignment (link to company goals), Expected Benefits (quantified), Success Criteria and Metrics |
| **Scope**             | In-Scope Items (specifically included), Out-of-Scope Items (explicitly excluded), Assumptions, Constraints |
| **Stakeholders**      | Name, Role, Interest Level, Engagement Strategy for each |
| **Key Deliverables**  | Deliverable, Due Date, Owner, Success Criteria |
| **Success Metrics/KPIs** | Metric, Baseline, Target, Measurement Frequency |
| **Team and Resources**| Name, Role, Time Commitment (per team member), Technology Requirements, Budget Allocation |
| **Risk Register**     | Risk Description, Likelihood, Impact, Mitigation Strategy, Owner |
| **Communication Plan**| Audience, Message, Frequency, Channel, Owner |
| **Approval Signatures** | Executive Sponsor, Project Manager, IT Lead, Finance Lead, Date |

## Quarterly Red-Teaming Checklist

| Month / Activity              | Checklist Items |
|-------------------------------|----------------|
| **Month 1: Assessment and Planning** | Review prior quarter's incidents<br>Update risk assessment (reflect new AI deployments)<br>Schedule red-teaming exercises<br>Audit marketing data privacy practices<br>Review consent management systems<br>Test privacy request fulfillment procedures |
| **Month 2: Active Testing**   | Red-teaming exercises (prompt injection, data poisoning, adversarial examples)<br>Test model performance and drift detection<br>Evaluate bias/fairness across demographics<br>Information integrity assessment (Harari, 2024)<br>Test incident response via tabletop exercise<br>Review third-party/vendor compliance |
| **Month 3: Review and Remediation** | Analyze results for patterns<br>Document findings/remediation steps with owners/due dates<br>Update protocols based on learnings<br>Refine containment strategies (Suleyman, 2023)<br>Prepare quarterly report (metrics/trends) for leadership<br>Plan next quarter's testing focus areas |
| **Continuous**                | Monitor AI system performance metrics daily<br>Log incidents in tracking system immediately<br>Weekly privacy rights request review<br>Monthly employee training on AI safety<br>Update documentation as systems change |

## Victory Journal Daily Template

| Field              | Prompts/Structure |
|--------------------|------------------|
| **Date**           |                  |
| **Morning (3 min)**| - Today's top 3 priorities<br>- How today advances our mission<br>- What obstacles might arise? |
| **Evening (4 min)**| - Victories (big and small)<br>- What worked well?<br>- Tomorrow's focus<br>- Key learning from today |

## Weekly Victory Sharing Circle Template

| Segment               | Duration | Activities |
|-----------------------|----------|------------|
| **Opening**           | 2 min    | Set purpose and context |
| **Victory Sharing**   | 20 min   | Each person shares 1–2 victories, brief context |
| **Team Celebration**  | 5 min    | Recognize patterns and collective wins |
| **Photo for Victory Wall** | 3 min | Take group photo to capture the moment |

## Sprint Demo Agenda Template

| Segment                 | Duration    | Activities |
|-------------------------|-------------|------------|
| **Context**             | 5 min       | Sprint goal reminder, key metrics from last sprint |
| **Demonstrations**      | 25 min      | 5 stories × 5 min: Acceptance criteria review, live user value demo, Q&A for each story |
| **Q&A and Feedback**    | 10 min      | Gather overall impressions and concerns |
| **Next Sprint Preview** | 5 min       | Preview upcoming priorities |

## Human-in-the-Loop Review Form (Google Forms)

| Field                | Type/Options |
|----------------------|--------------|
| Reviewer Name        | Dropdown     |
| Item ID              | Short answer |
| AI Output Summary    | Paragraph    |
| AI Confidence Score  | Linear scale (0–100) |
| Approval Decision    | Multiple choice (Approve / Reject / Request Changes) |
| Issues Found         | Checkboxes (Accuracy, Completeness, Compliance, Bias, Other) |
| Specific Feedback    | Paragraph    |
| Recommended Action   | Paragraph    |
| Review Time Spent    | Short answer (minutes) |
| Escalation Needed    | Yes/No (if yes, explanation required) |

## 30-60-90 Day Success Metrics Dashboard

| Metric Group      | Metrics/Fields |
|-------------------|----------------|
| **Pilot Program Health** | Days elapsed, Current phase, On track / At risk / Off track, Blockers count, Risk count |
| **Adoption Metrics**     | Pilot users trained, Daily active users, Adoption rate (%), Feature utilization rate |
| **Performance Metrics**  | AI accuracy/confidence, System uptime (%), Avg. response time, Errors logged count |
| **Business Impact**      | Time saved/user weekly, Cost savings, User satisfaction score, Productivity improvement (%) |
| **Milestones**           | Milestone, Target date, Actual date, Status, Notes |

*Review this dashboard weekly with the pilot team, bi-weekly with stakeholders, and monthly with leadership.*

## Quarterly OKR Template

| Field                   | Structure/Prompts |
|-------------------------|-------------------|
| Objective               | Clear, qualitative statement of achievement |
| Key Results             | 3–5 quantitative measures of success |
| Owner                   | Person accountable |
| Quarter                 | Q1/Q2/Q3/Q4 Year |
| Current Status          | On track / At risk / Off track |
| Overall Score           | 0.0–1.0 |
| Individual KR Details   | Description, Baseline, Target, Actual, Score (0.0–1.0), Status |
| Progress Updates        | Date, Update, Challenges, Next steps |
| Retrospective           | What worked well, What to improve, Learnings for next quarter |

**Score 0.7–0.8 is target range; consistently scoring 1.0 means OKRs aren't ambitious enough.**
